Complete Mathematical Specification: Learned Memory Addressing
ğŸ“ Mathematical Formulation
Problem Setup
Given:

Event sequence: E={e1,e2,...,eT}\mathcal{E} = \{e_1, e_2, ..., e_T\}
E={e1â€‹,e2â€‹,...,eTâ€‹} where et=(vt,st,Ï„t)e_t = (v_t, s_t, \tau_t)
etâ€‹=(vtâ€‹,stâ€‹,Ï„tâ€‹)

vtv_t
vtâ€‹: visual observation (image/screenshot)

sts_t
stâ€‹: symbolic state (app, entities, activity)

Ï„t\tau_t
Ï„tâ€‹: timestamp



Memory bank: Mt={m1,m2,...,mNt}\mathcal{M}_t = \{m_1, m_2, ..., m_{N_t}\}
Mtâ€‹={m1â€‹,m2â€‹,...,mNtâ€‹â€‹} where mi=(xi,Ïƒi,Ï„iupdate,ci)m_i = (x_i, \sigma_i, \tau_i^{\text{update}}, c_i)
miâ€‹=(xiâ€‹,Ïƒiâ€‹,Ï„iupdateâ€‹,ciâ€‹)

xix_i
xiâ€‹: memory text

Ïƒi\sigma_i
Ïƒiâ€‹: memory state metadata

Ï„iupdate\tau_i^{\text{update}}
Ï„iupdateâ€‹: last update time

cic_i
ciâ€‹: update count




Goal: Learn projections Î¦,Î¨\Phi, \Psi
Î¦,Î¨ such that for each event ete_t
etâ€‹, we identify memories Mtâˆ—âŠ‚Mt\mathcal{M}_t^* \subset \mathcal{M}_t
Mtâˆ—â€‹âŠ‚Mtâ€‹ that should be updated via operations O={UPDATE,MERGE,DELETE,IGNORE}\mathcal{O} = \{\text{UPDATE}, \text{MERGE}, \text{DELETE}, \text{IGNORE}\}
O={UPDATE,MERGE,DELETE,IGNORE}.


ğŸ—ï¸ Model Architecture
1. Event Encoder: Î¦(et,Ï„now)â†’Rd\Phi(e_t, \tau_{\text{now}}) \rightarrow \mathbb{R}^d
Î¦(etâ€‹,Ï„nowâ€‹)â†’Rd
fimg=CLIPfrozen(vt)âˆˆR512fsym=Embed(st)âˆˆR64ftime=T(et,Ï„now)âˆˆR14he=[fimg;fsym;ftime]âˆˆR590ze=MLPÎ¦(he)âˆˆR64\begin{align}
\mathbf{f}_{\text{img}} &= \text{CLIP}_{\text{frozen}}(v_t) \in \mathbb{R}^{512} \\
\mathbf{f}_{\text{sym}} &= \text{Embed}(s_t) \in \mathbb{R}^{64} \\
\mathbf{f}_{\text{time}} &= \mathcal{T}(e_t, \tau_{\text{now}}) \in \mathbb{R}^{14} \\
\mathbf{h}_e &= [\mathbf{f}_{\text{img}}; \mathbf{f}_{\text{sym}}; \mathbf{f}_{\text{time}}] \in \mathbb{R}^{590} \\
\mathbf{z}_e &= \text{MLP}_\Phi(\mathbf{h}_e) \in \mathbb{R}^{64}
\end{align}fimgâ€‹fsymâ€‹ftimeâ€‹heâ€‹zeâ€‹â€‹=CLIPfrozenâ€‹(vtâ€‹)âˆˆR512=Embed(stâ€‹)âˆˆR64=T(etâ€‹,Ï„nowâ€‹)âˆˆR14=[fimgâ€‹;fsymâ€‹;ftimeâ€‹]âˆˆR590=MLPÎ¦â€‹(heâ€‹)âˆˆR64â€‹â€‹
Where temporal encoding T(et,Ï„now)\mathcal{T}(e_t, \tau_{\text{now}})
T(etâ€‹,Ï„nowâ€‹) is:

T(et,Ï„now)=[sinâ¡(2Ï€h/24),cosâ¡(2Ï€h/24)sinâ¡(2Ï€dw/7),cosâ¡(2Ï€dw/7)sinâ¡(2Ï€m/12),cosâ¡(2Ï€m/12)logâ¡(1+Î”min),logâ¡(1+Î”hr)logâ¡(1+Î”day),logâ¡(1+Î”wk),logâ¡(1+Î”mo)]\mathcal{T}(e_t, \tau_{\text{now}}) = \left[\begin{array}{c}
\sin(2\pi h / 24), \cos(2\pi h / 24) \\
\sin(2\pi d_w / 7), \cos(2\pi d_w / 7) \\
\sin(2\pi m / 12), \cos(2\pi m / 12) \\
\log(1 + \Delta_{\text{min}}), \log(1 + \Delta_{\text{hr}}) \\
\log(1 + \Delta_{\text{day}}), \log(1 + \Delta_{\text{wk}}), \log(1 + \Delta_{\text{mo}})
\end{array}\right]T(etâ€‹,Ï„nowâ€‹)=â€‹sin(2Ï€h/24),cos(2Ï€h/24)sin(2Ï€dwâ€‹/7),cos(2Ï€dwâ€‹/7)sin(2Ï€m/12),cos(2Ï€m/12)log(1+Î”minâ€‹),log(1+Î”hrâ€‹)log(1+Î”dayâ€‹),log(1+Î”wkâ€‹),log(1+Î”moâ€‹)â€‹â€‹
where Î”min=(Ï„nowâˆ’Ï„t)/60\Delta_{\text{min}} = (\tau_{\text{now}} - \tau_t) / 60
Î”minâ€‹=(Ï„nowâ€‹âˆ’Ï„tâ€‹)/60, etc.

MLP architecture:
MLPÎ¦=GELU(W3â‹…GELU(W2â‹…GELU(W1he)))\text{MLP}_\Phi = \text{GELU}(\mathbf{W}_3 \cdot \text{GELU}(\mathbf{W}_2 \cdot \text{GELU}(\mathbf{W}_1 \mathbf{h}_e)))MLPÎ¦â€‹=GELU(W3â€‹â‹…GELU(W2â€‹â‹…GELU(W1â€‹heâ€‹)))
W1âˆˆR256Ã—590,W2âˆˆR128Ã—256,W3âˆˆR64Ã—128\mathbf{W}_1 \in \mathbb{R}^{256 \times 590}, \quad \mathbf{W}_2 \in \mathbb{R}^{128 \times 256}, \quad \mathbf{W}_3 \in \mathbb{R}^{64 \times 128}W1â€‹âˆˆR256Ã—590,W2â€‹âˆˆR128Ã—256,W3â€‹âˆˆR64Ã—128

2. Memory Encoder: Î¨(mi,Ï„now)â†’Rd\Psi(m_i, \tau_{\text{now}}) \rightarrow \mathbb{R}^d
Î¨(miâ€‹,Ï„nowâ€‹)â†’Rd
ftxt=SentenceT5(xi)âˆˆR384fmeta=[logâ¡(1+ci),logâ¡(1+Î”age),impi]âˆˆR3hm=[ftxt;fmeta]âˆˆR387zm=MLPÎ¨(hm)âˆˆR64\begin{align}
\mathbf{f}_{\text{txt}} &= \text{SentenceT5}(x_i) \in \mathbb{R}^{384} \\
\mathbf{f}_{\text{meta}} &= [\log(1 + c_i), \log(1 + \Delta_{\text{age}}), \text{imp}_i] \in \mathbb{R}^3 \\
\mathbf{h}_m &= [\mathbf{f}_{\text{txt}}; \mathbf{f}_{\text{meta}}] \in \mathbb{R}^{387} \\
\mathbf{z}_m &= \text{MLP}_\Psi(\mathbf{h}_m) \in \mathbb{R}^{64}
\end{align}ftxtâ€‹fmetaâ€‹hmâ€‹zmâ€‹â€‹=SentenceT5(xiâ€‹)âˆˆR384=[log(1+ciâ€‹),log(1+Î”ageâ€‹),impiâ€‹]âˆˆR3=[ftxtâ€‹;fmetaâ€‹]âˆˆR387=MLPÎ¨â€‹(hmâ€‹)âˆˆR64â€‹â€‹
where Î”age=(Ï„nowâˆ’Ï„iupdate)/86400\Delta_{\text{age}} = (\tau_{\text{now}} - \tau_i^{\text{update}}) / 86400
Î”ageâ€‹=(Ï„nowâ€‹âˆ’Ï„iupdateâ€‹)/86400 (days).

Critical: MLPÎ¦â‰ MLPÎ¨\text{MLP}_\Phi \neq \text{MLP}_\Psi
MLPÎ¦â€‹î€ =MLPÎ¨â€‹ (asymmetric encoders).

3. Scoring Function
s(et,mi)=zeâŠ¤zmâˆ¥zeâˆ¥âˆ¥zmâˆ¥+Î±â‹…temp_weight(Ï„t,Ï„iupdate,Ï„now)s(e_t, m_i) = \frac{\mathbf{z}_e^\top \mathbf{z}_m}{\|\mathbf{z}_e\| \|\mathbf{z}_m\|} + \alpha \cdot \text{temp\_weight}(\tau_t, \tau_i^{\text{update}}, \tau_{\text{now}})s(etâ€‹,miâ€‹)=âˆ¥zeâ€‹âˆ¥âˆ¥zmâ€‹âˆ¥zeâŠ¤â€‹zmâ€‹â€‹+Î±â‹…temp_weight(Ï„tâ€‹,Ï„iupdateâ€‹,Ï„nowâ€‹)
where:

temp_weight(Ï„t,Ï„m,Ï„now)=expâ¡(âˆ’âˆ£(Ï„nowâˆ’Ï„t)âˆ’(Ï„nowâˆ’Ï„m)âˆ£7â‹…86400)\text{temp\_weight}(\tau_t, \tau_m, \tau_{\text{now}}) = \exp\left(-\frac{|(\tau_{\text{now}} - \tau_t) - (\tau_{\text{now}} - \tau_m)|}{7 \cdot 86400}\right)temp_weight(Ï„tâ€‹,Ï„mâ€‹,Ï„nowâ€‹)=exp(âˆ’7â‹…86400âˆ£(Ï„nowâ€‹âˆ’Ï„tâ€‹)âˆ’(Ï„nowâ€‹âˆ’Ï„mâ€‹)âˆ£â€‹)
This gives bonus to memories whose age matches the event's recency.

4. Operation Classifier (Multi-Task Head)
zpair=[ze;zm;zeâŠ™zm;âˆ£zeâˆ’zmâˆ£]âˆˆR256p(Oâˆ£et,mi)=softmax(Wopâ‹…ReLU(Wpairzpair))\begin{align}
\mathbf{z}_{\text{pair}} &= [\mathbf{z}_e; \mathbf{z}_m; \mathbf{z}_e \odot \mathbf{z}_m; |\mathbf{z}_e - \mathbf{z}_m|] \in \mathbb{R}^{256} \\
p(\mathcal{O} | e_t, m_i) &= \text{softmax}(\mathbf{W}_{\text{op}} \cdot \text{ReLU}(\mathbf{W}_{\text{pair}} \mathbf{z}_{\text{pair}}))
\end{align}zpairâ€‹p(Oâˆ£etâ€‹,miâ€‹)â€‹=[zeâ€‹;zmâ€‹;zeâ€‹âŠ™zmâ€‹;âˆ£zeâ€‹âˆ’zmâ€‹âˆ£]âˆˆR256=softmax(Wopâ€‹â‹…ReLU(Wpairâ€‹zpairâ€‹))â€‹â€‹
where WpairâˆˆR64Ã—256\mathbf{W}_{\text{pair}} \in \mathbb{R}^{64 \times 256}
Wpairâ€‹âˆˆR64Ã—256, WopâˆˆR4Ã—64\mathbf{W}_{\text{op}} \in \mathbb{R}^{4 \times 64}
Wopâ€‹âˆˆR4Ã—64 (4 operations).


ğŸ¯ Training Objective
Total Loss
Ltotal=Lrank+Î»mLmargin+Î»oLop+Î»cLconsist\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{rank}} + \lambda_m \mathcal{L}_{\text{margin}} + \lambda_o \mathcal{L}_{\text{op}} + \lambda_c \mathcal{L}_{\text{consist}}Ltotalâ€‹=Lrankâ€‹+Î»mâ€‹Lmarginâ€‹+Î»oâ€‹Lopâ€‹+Î»câ€‹Lconsistâ€‹

Loss 1: Multi-Positive Contrastive Ranking
For event ete_t
etâ€‹ with positive memories Mt+={mj:ojâˆˆ{UPDATE,MERGE}}\mathcal{M}_t^+ = \{m_j : o_j \in \{\text{UPDATE}, \text{MERGE}\}\}
Mt+â€‹={mjâ€‹:ojâ€‹âˆˆ{UPDATE,MERGE}} and negatives Mtâˆ’\mathcal{M}_t^-
Mtâˆ’â€‹:

Lrank=âˆ’logâ¡âˆ‘mjâˆˆMt+expâ¡(s(et,mj)/Ï„)âˆ‘mjâˆˆMt+expâ¡(s(et,mj)/Ï„)+âˆ‘mkâˆˆMtâˆ’expâ¡(s(et,mk)/Ï„)\mathcal{L}_{\text{rank}} = -\log \frac{\sum_{m_j \in \mathcal{M}_t^+} \exp(s(e_t, m_j)/\tau)}{\sum_{m_j \in \mathcal{M}_t^+} \exp(s(e_t, m_j)/\tau) + \sum_{m_k \in \mathcal{M}_t^-} \exp(s(e_t, m_k)/\tau)}Lrankâ€‹=âˆ’logâˆ‘mjâ€‹âˆˆMt+â€‹â€‹exp(s(etâ€‹,mjâ€‹)/Ï„)+âˆ‘mkâ€‹âˆˆMtâˆ’â€‹â€‹exp(s(etâ€‹,mkâ€‹)/Ï„)âˆ‘mjâ€‹âˆˆMt+â€‹â€‹exp(s(etâ€‹,mjâ€‹)/Ï„)â€‹
where Ï„=0.07\tau = 0.07
Ï„=0.07 (temperature).

Negative sampling strategy:
Mtâˆ’=Mtretrievedâˆ–Mt+âˆªMthard\mathcal{M}_t^- = \mathcal{M}_t^{\text{retrieved}} \setminus \mathcal{M}_t^+ \cup \mathcal{M}_t^{\text{hard}}Mtâˆ’â€‹=Mtretrievedâ€‹âˆ–Mt+â€‹âˆªMthardâ€‹
where:

Mtretrieved\mathcal{M}_t^{\text{retrieved}}
Mtretrievedâ€‹: top-K from previous retrieval (ignored by LLM)

Mthard\mathcal{M}_t^{\text{hard}}
Mthardâ€‹: high similarity but wrong temporal context



Loss 2: Hard Negative Margin Loss
Lmargin=âˆ‘mjâˆˆMt+âˆ‘mkâˆˆMthardmaxâ¡(0,Î³+s(et,mk)âˆ’s(et,mj))\mathcal{L}_{\text{margin}} = \sum_{m_j \in \mathcal{M}_t^+} \sum_{m_k \in \mathcal{M}_t^{\text{hard}}} \max(0, \gamma + s(e_t, m_k) - s(e_t, m_j))Lmarginâ€‹=mjâ€‹âˆˆMt+â€‹âˆ‘â€‹mkâ€‹âˆˆMthardâ€‹âˆ‘â€‹max(0,Î³+s(etâ€‹,mkâ€‹)âˆ’s(etâ€‹,mjâ€‹))
where Î³=0.2\gamma = 0.2
Î³=0.2 (margin).


Loss 3: Operation Classification
Lop=âˆ’âˆ‘miâˆˆMtcandidateslogâ¡p(oiâˆ—âˆ£et,mi)\mathcal{L}_{\text{op}} = -\sum_{m_i \in \mathcal{M}_t^{\text{candidates}}} \log p(o_i^* | e_t, m_i)Lopâ€‹=âˆ’miâ€‹âˆˆMtcandidatesâ€‹âˆ‘â€‹logp(oiâˆ—â€‹âˆ£etâ€‹,miâ€‹)
where oiâˆ—o_i^*
oiâˆ—â€‹ is the ground truth operation from LLM.


Loss 4: Memory Consistency Regularization
For memories sharing entities G={(mi,mj):entity(mi)âˆ©entity(mj)â‰ âˆ…}\mathcal{G} = \{(m_i, m_j) : \text{entity}(m_i) \cap \text{entity}(m_j) \neq \emptyset\}
G={(miâ€‹,mjâ€‹):entity(miâ€‹)âˆ©entity(mjâ€‹)î€ =âˆ…}:

Lconsist=âˆ‘(mi,mj)âˆˆGâˆ¥zmiâˆ’zmjâˆ¥22â‹…1[oi=oj=UPDATE]\mathcal{L}_{\text{consist}} = \sum_{(m_i, m_j) \in \mathcal{G}} \|\mathbf{z}_{m_i} - \mathbf{z}_{m_j}\|_2^2 \cdot \mathbb{1}[o_i = o_j = \text{UPDATE}]Lconsistâ€‹=(miâ€‹,mjâ€‹)âˆˆGâˆ‘â€‹âˆ¥zmiâ€‹â€‹âˆ’zmjâ€‹â€‹âˆ¥22â€‹â‹…1[oiâ€‹=ojâ€‹=UPDATE]
This encourages related memories to have similar representations when both get updated.

ğŸ”„ Training Algorithm
Phase 1: Offline Supervised Training
python# Hyperparameters
d_model = 64
batch_size = 128
num_epochs = 50
learning_rate = 3e-4
temperature = 0.07
margin = 0.2
Î»_m, Î»_o, Î»_c = 0.2, 0.3, 0.1

# Initialize models
Î¦ = EventEncoder(d_model)
Î¨ = MemoryEncoder(d_model)
OpClassifier = OperationHead(d_model)

optimizer = AdamW([Î¦.parameters(), Î¨.parameters(), 
                   OpClassifier.parameters()], lr=learning_rate)
scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)

# Training loop
for epoch in range(num_epochs):
    for batch in dataloader:
        events, memories, labels = batch
        # labels = {
        #   'pos_indices': [...],      # memories to update
        #   'neg_indices': [...],      # retrieved but ignored
        #   'hard_neg_indices': [...], # semantic near-misses
        #   'operations': [...],       # ground truth ops
        #   'entity_graph': [...]      # co-occurrence pairs
        # }
        
        # === Forward pass ===
        z_events = Î¦(events.img, events.symbols, events.time, Ï„_now)  # [B, 64]
        z_mems = Î¨(memories.text, memories.meta, Ï„_now)                # [M, 64]
        
        # Compute similarities
        scores = (z_events @ z_mems.T) / (
            torch.norm(z_events, dim=1, keepdim=True) * 
            torch.norm(z_mems, dim=1, keepdim=True).T
        )  # [B, M]
        
        # Add temporal weighting
        temporal_weights = compute_temporal_weights(
            events.time, memories.update_time, Ï„_now
        )  # [B, M]
        scores = scores + 0.1 * temporal_weights
        
        # === Loss 1: Contrastive ranking ===
        L_rank = 0
        for i in range(batch_size):
            pos_mask = labels['pos_indices'][i]  # [M] binary
            neg_mask = labels['neg_indices'][i] | labels['hard_neg_indices'][i]
            
            pos_scores = scores[i][pos_mask]
            neg_scores = scores[i][neg_mask]
            
            numerator = torch.sum(torch.exp(pos_scores / temperature))
            denominator = numerator + torch.sum(torch.exp(neg_scores / temperature))
            
            L_rank += -torch.log(numerator / denominator)
        
        L_rank = L_rank / batch_size
        
        # === Loss 2: Hard negative margin ===
        L_margin = 0
        for i in range(batch_size):
            pos_scores = scores[i][labels['pos_indices'][i]]
            hard_neg_scores = scores[i][labels['hard_neg_indices'][i]]
            
            for pos_s in pos_scores:
                for neg_s in hard_neg_scores:
                    L_margin += torch.relu(margin + neg_s - pos_s)
        
        L_margin = L_margin / batch_size
        
        # === Loss 3: Operation classification ===
        op_logits = OpClassifier(z_events, z_mems)  # [B, M, 4]
        L_op = F.cross_entropy(
            op_logits.view(-1, 4),
            labels['operations'].view(-1)
        )
        
        # === Loss 4: Consistency regularization ===
        L_consist = 0
        for (i, j) in labels['entity_graph']:
            if labels['operations'][i] == labels['operations'][j] == OP_UPDATE:
                L_consist += torch.norm(z_mems[i] - z_mems[j], p=2)**2
        
        L_consist = L_consist / len(labels['entity_graph']) if labels['entity_graph'] else 0
        
        # === Combine losses ===
        loss = L_rank + Î»_m * L_margin + Î»_o * L_op + Î»_c * L_consist
        
        # === Backward pass ===
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_([Î¦.parameters(), Î¨.parameters()], max_norm=1.0)
        optimizer.step()
        
        # Logging
        if step % 100 == 0:
            print(f"Epoch {epoch}, Step {step}: L_total={loss:.4f}, "
                  f"L_rank={L_rank:.4f}, L_margin={L_margin:.4f}, "
                  f"L_op={L_op:.4f}, L_consist={L_consist:.4f}")
    
    scheduler.step()
    
    # === Evaluation ===
    if epoch % 5 == 0:
        evaluate(Î¦, Î¨, OpClassifier, val_loader)

Phase 2: Online Adaptation
python# Online fine-tuning with replay buffer
replay_buffer = ReplayBuffer(max_size=10000)
ema_Î¦ = copy.deepcopy(Î¦)  # exponential moving average
ema_Î¨ = copy.deepcopy(Î¨)
ema_decay = 0.999

for t, event in enumerate(event_stream):
    # === Inference (use EMA models) ===
    with torch.no_grad():
        z_e = ema_Î¦(event.img, event.symbols, event.time, Ï„_now)
        z_m = ema_Î¨(memory_bank.text, memory_bank.meta, Ï„_now)
        
        scores = z_e @ z_m.T
        top_k_indices = torch.topk(scores, k=5).indices
    
    # === Collect LLM feedback (sparse) ===
    if random.random() < 0.1:  # 10% sampling rate
        ground_truth = query_LLM(event, memory_bank[top_k_indices])
        replay_buffer.add(event, memory_bank[top_k_indices], ground_truth)
    
    # === Periodic retraining ===
    if len(replay_buffer) >= 1000 and t % 1000 == 0:
        print(f"Retraining at step {t}...")
        
        for mini_epoch in range(10):
            batch = replay_buffer.sample(batch_size=64)
            
            # Same training loop as Phase 1
            loss = compute_loss(Î¦, Î¨, OpClassifier, batch)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Update EMA models
            for param_ema, param in zip(ema_Î¦.parameters(), Î¦.parameters()):
                param_ema.data = ema_decay * param_ema.data + (1 - ema_decay) * param.data
            
            for param_ema, param in zip(ema_Î¨.parameters(), Î¨.parameters()):
                param_ema.data = ema_decay * param_ema.data + (1 - ema_decay) * param.data

ğŸš€ Inference Algorithm
pythondef inference(event, memory_bank, Î¦, Î¨, OpClassifier, Ï„_now, K=5):
    """
    Args:
        event: current observation
        memory_bank: all memories [N memories]
        K: number of memories to pass to LLM
    
    Returns:
        selected_memories: indices of top-K memories
        predicted_ops: predicted operations for each
    """
    
    # === Step 1: Encode event ===
    with torch.no_grad():
        z_event = Î¦(event.img, event.symbols, event.time, Ï„_now)  # [64]
        
        # === Step 2: Encode all memories ===
        z_memories = Î¨(
            memory_bank.text, 
            memory_bank.meta, 
            Ï„_now
        )  # [N, 64]
        
        # === Step 3: Compute time-aware scores ===
        # Semantic similarity
        semantic_scores = (z_event @ z_memories.T) / (
            torch.norm(z_event) * torch.norm(z_memories, dim=1)
        )  # [N]
        
        # Temporal relevance
        temporal_scores = compute_temporal_weights(
            event.time,
            memory_bank.update_time,
            Ï„_now
        )  # [N]
        
        # Combined score
        final_scores = semantic_scores + 0.1 * temporal_scores  # [N]
        
        # === Step 4: Select top-K ===
        top_k_values, top_k_indices = torch.topk(final_scores, k=K)
        
        # === Step 5: Predict operations ===
        selected_memories = memory_bank[top_k_indices]
        z_selected = z_memories[top_k_indices]  # [K, 64]
        
        predicted_ops = OpClassifier(
            z_event.unsqueeze(0).expand(K, -1),  # [K, 64]
            z_selected                            # [K, 64]
        )  # [K, 4]
        
        predicted_ops = torch.argmax(predicted_ops, dim=-1)  # [K]
        
        # === Step 6: Filter by confidence ===
        # Only pass to LLM if operation is likely UPDATE/MERGE
        mask = (predicted_ops == OP_UPDATE) | (predicted_ops == OP_MERGE)
        confident_indices = top_k_indices[mask]
        
        if len(confident_indices) == 0:
            # No confident updates, retrieve top-3 anyway
            confident_indices = top_k_indices[:3]
    
    return confident_indices, predicted_ops


def compute_temporal_weights(event_time, memory_times, current_time):
    """
    Compute temporal alignment between event and memories.
    
    Returns:
        weights: [N] - higher for temporally aligned memories
    """
    event_age = (current_time - event_time).total_seconds()
    memory_ages = (current_time - memory_times).total_seconds()
    
    gaps = torch.abs(event_age - memory_ages)
    
    # 7-day half-life
    weights = torch.exp(-gaps / (7 * 86400))
    
    return weights

ğŸ“Š Evaluation Metrics
Metric 1: Update Precision@K
Prec@K=1âˆ£Eâˆ£âˆ‘etâˆˆEâˆ£Mtpredâˆ©Mtâˆ—âˆ£K\text{Prec@K} = \frac{1}{|\mathcal{E}|} \sum_{e_t \in \mathcal{E}} \frac{|\mathcal{M}_t^{\text{pred}} \cap \mathcal{M}_t^*|}{K}Prec@K=âˆ£Eâˆ£1â€‹etâ€‹âˆˆEâˆ‘â€‹Kâˆ£Mtpredâ€‹âˆ©Mtâˆ—â€‹âˆ£â€‹
where Mtpred\mathcal{M}_t^{\text{pred}}
Mtpredâ€‹ are predicted top-K memories, Mtâˆ—\mathcal{M}_t^*
Mtâˆ—â€‹ are ground truth.


Metric 2: Update Recall@K
Rec@K=1âˆ£Eâˆ£âˆ‘etâˆˆEâˆ£Mtpredâˆ©Mtâˆ—âˆ£âˆ£Mtâˆ—âˆ£\text{Rec@K} = \frac{1}{|\mathcal{E}|} \sum_{e_t \in \mathcal{E}} \frac{|\mathcal{M}_t^{\text{pred}} \cap \mathcal{M}_t^*|}{|\mathcal{M}_t^*|}Rec@K=âˆ£Eâˆ£1â€‹etâ€‹âˆˆEâˆ‘â€‹âˆ£Mtâˆ—â€‹âˆ£âˆ£Mtpredâ€‹âˆ©Mtâˆ—â€‹âˆ£â€‹

Metric 3: Operation Accuracy
OpAcc=1âˆ‘tâˆ£Mtâˆ—âˆ£âˆ‘etâˆ‘miâˆˆMtâˆ—1[oipred=oiâˆ—]\text{OpAcc} = \frac{1}{\sum_t |\mathcal{M}_t^*|} \sum_{e_t} \sum_{m_i \in \mathcal{M}_t^*} \mathbb{1}[o_i^{\text{pred}} = o_i^*]OpAcc=âˆ‘tâ€‹âˆ£Mtâˆ—â€‹âˆ£1â€‹etâ€‹âˆ‘â€‹miâ€‹âˆˆMtâˆ—â€‹âˆ‘â€‹1[oipredâ€‹=oiâˆ—â€‹]

Metric 4: Memory Health
Redundancy:
Redundancy=1N(Nâˆ’1)âˆ‘iâ‰ jsim(mi,mj)\text{Redundancy} = \frac{1}{N(N-1)} \sum_{i \neq j} \text{sim}(m_i, m_j)Redundancy=N(Nâˆ’1)1â€‹iî€ =jâˆ‘â€‹sim(miâ€‹,mjâ€‹)
Contradiction Rate:
Contradiction=âˆ£{(mi,mj):LLM-detects-conflict(mi,mj)}âˆ£N(Nâˆ’1)/2\text{Contradiction} = \frac{|\{(m_i, m_j) : \text{LLM-detects-conflict}(m_i, m_j)\}|}{N(N-1)/2}Contradiction=N(Nâˆ’1)/2âˆ£{(miâ€‹,mjâ€‹):LLM-detects-conflict(miâ€‹,mjâ€‹)}âˆ£â€‹

Metric 5: LLM Efficiency
EfficiencyÂ Gain=1âˆ’KlearnedNtotal\text{Efficiency Gain} = 1 - \frac{K_{\text{learned}}}{N_{\text{total}}}EfficiencyÂ Gain=1âˆ’Ntotalâ€‹Klearnedâ€‹â€‹
where Klearnedâ‰ªNtotalK_{\text{learned}} \ll N_{\text{total}}
Klearnedâ€‹â‰ªNtotalâ€‹.


ğŸ”¬ Data Generation
pythondef generate_training_data(user_events, LLM, memory_bank):
    """
    Generate supervised training data from LLM interactions.
    
    Returns:
        dataset: [(event, positive_mems, negative_mems, operations)]
    """
    dataset = []
    
    for event in user_events:
        # Initial retrieval (can be heuristic)
        candidates = retrieve_candidates(event, memory_bank, K=20)
        
        # Query LLM for ground truth
        llm_response = LLM.process(event, candidates)
        # llm_response = {
        #   'updates': [m3, m7],      # memories to update
        #   'merges': [(m1, m5)],     # memories to merge
        #   'deletes': [m9],          # memories to delete
        #   'ignores': [m2, m4, ...], # others
        # }
        
        # Construct labels
        positive_mems = llm_response['updates'] + [m for pair in llm_response['merges'] for m in pair]
        negative_mems = llm_response['ignores']
        
        # Hard negatives: high similarity but ignored
        hard_negatives = [
            m for m in negative_mems 
            if semantic_similarity(event, m) > 0.7
        ]
        
        operations = {}
        for m in llm_response['updates']:
            operations[m.id] = OP_UPDATE
        for pair in llm_response['merges']:
            operations[pair[0].id] = OP_MERGE
            operations[pair[1].id] = OP_MERGE
        for m in llm_response['deletes']:
            operations[m.id] = OP_DELETE
        for m in llm_response['ignores']:
            operations[m.id] = OP_IGNORE
        
        dataset.append({
            'event': event,
            'positive_memories': positive_mems,
            'negative_memories': negative_mems,
            'hard_negatives': hard_negatives,
            'operations': operations,
        })
    
    return dataset
```

---

## ğŸ¯ **Complete System Pipeline**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRAINING PHASE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Collect user events E = {eâ‚, eâ‚‚, ..., eâ‚œ}              â”‚
â”‚  2. For each event:                                          â”‚
â”‚     a. Heuristic retrieval â†’ K=20 candidates                â”‚
â”‚     b. LLM decides operations â†’ ground truth labels         â”‚
â”‚     c. Extract (event, pos_mems, neg_mems, ops)            â”‚
â”‚  3. Train Î¦, Î¨, OpClassifier with multi-loss               â”‚
â”‚     L = L_rank + Î»â‚˜L_margin + Î»â‚’L_op + Î»êœ€L_consist          â”‚
â”‚  4. Validate on held-out users                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INFERENCE PHASE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  New event eâ‚œ arrives:                                       â”‚
â”‚                                                              â”‚
â”‚  1. Encode: z_e = Î¦(eâ‚œ, Ï„_now)                             â”‚
â”‚  2. Encode all memories: Z_m = Î¨(M, Ï„_now)                 â”‚
â”‚  3. Score: s = (z_e âŠ™ Z_m) + Î±Â·temporal_weight             â”‚
â”‚  4. Select top-K indices                                     â”‚
â”‚  5. Predict ops: OpClassifier(z_e, Z_m[top_K])             â”‚
â”‚  6. Filter confident updates â†’ K'â‰¤K memories                â”‚
â”‚  7. Pass K' memories to LLM for final decision              â”‚
â”‚  8. Execute memory operations                                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ K'=3-5   â”‚ â†’  â”‚ LLM  â”‚ â†’  â”‚ Execâ”‚                       â”‚
â”‚  â”‚ memories â”‚    â”‚      â”‚    â”‚     â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                              â”‚
â”‚  Efficiency: 3-5 vs 100+ memories to LLM                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ONLINE ADAPTATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Every 1000 steps:                                          â”‚
â”‚  1. Sample 10% events for LLM verification                  â”‚
â”‚  2. Add to replay buffer                                     â”‚
â”‚  3. Retrain Î¦, Î¨ on buffer (10 mini-epochs)                â”‚
â”‚  4. Update EMA models                                        â”‚
â”‚  5. Continue inference with EMA                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¦ Model Sizes
ComponentParametersMemoryCLIP (frozen)0 (shared)0 MBSentenceT5 (frozen)0 (shared)0 MBEvent Encoder Î¦~380K1.5 MBMemory Encoder Î¨~250K1.0 MBOp Classifier~20K0.1 MBTotal trainable~650K2.6 MB
This is extremely lightweight compared to training a full LLM.

âœ… Key Innovations

Asymmetric encoders: Î¦â‰ Î¨\Phi \neq \Psi
Î¦î€ =Î¨ for update vs memory representation

Time-aware scoring: Explicit temporal alignment in similarity
Multi-positive contrastive: Handles multiple memories per event
Operation prediction: Joint training for routing + action classification
Consistency regularization: Co-update related memories
EMA for online learning: Stable adaptation without catastrophic forgetting

This is a complete, implementable system ready for experimentation.CLIP is too expensive to run on phone. Add alternatives. Also explain the intuition behind having the loss function or explain why to have this kind of loss. Also add details about training data generation.10:12 PMComplete Mathematical Specification: Learned Memory Addressing
(Mobile-Optimized Version)

ğŸ“± Mobile-Friendly Vision Encoders
Problem with CLIP

CLIP ViT-B/32: ~150M params, ~600MB memory, ~500ms latency on phone
Battery drain, thermal throttling, poor UX

Mobile Alternatives
Option 1: MobileNetV3-Small (Recommended)
pythonimport torchvision.models as models

class MobileVisionEncoder(nn.Module):
    def __init__(self, output_dim=128):
        super().__init__()
        # Load pretrained MobileNetV3
        backbone = models.mobilenet_v3_small(pretrained=True)
        
        # Remove classifier, keep features
        self.features = backbone.features  # ~1.5M params
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.proj = nn.Linear(576, output_dim)  # 576 is MobileNetV3-small output
        
    def forward(self, x):
        # x: [B, 3, 224, 224]
        x = self.features(x)  # [B, 576, 7, 7]
        x = self.pool(x)       # [B, 576, 1, 1]
        x = x.flatten(1)       # [B, 576]
        return self.proj(x)    # [B, 128]

# Stats:
# - Params: ~1.5M (100Ã— smaller than CLIP)
# - Memory: ~6MB
# - Latency: ~20ms on modern phones
# - Accuracy: 67% ImageNet top-1 (good enough for scene recognition)
Option 2: EfficientNet-Lite0
python# Even more efficient for very low-end devices
from efficientnet_lite import EfficientNetLite0

class EfficientVisionEncoder(nn.Module):
    def __init__(self, output_dim=128):
        super().__init__()
        self.backbone = EfficientNetLite0(pretrained=True)
        self.backbone.classifier = nn.Identity()  # Remove head
        self.proj = nn.Linear(1280, output_dim)
        
    def forward(self, x):
        x = self.backbone(x)  # [B, 1280]
        return self.proj(x)    # [B, 128]

# Stats:
# - Params: ~4.6M
# - Memory: ~18MB  
# - Latency: ~30ms
# - Accuracy: 75% ImageNet top-1
Option 3: Custom Tiny CNN (Ultra-lightweight)
pythonclass TinyVisionEncoder(nn.Module):
    """
    For ultra-constrained devices (old phones, embedded systems)
    """
    def __init__(self, output_dim=128):
        super().__init__()
        self.conv = nn.Sequential(
            # Block 1: 224Ã—224 â†’ 56Ã—56
            nn.Conv2d(3, 16, 7, stride=4, padding=3),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            # Block 2: 56Ã—56 â†’ 28Ã—28
            nn.Conv2d(16, 32, 3, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # Block 3: 28Ã—28 â†’ 14Ã—14
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # Block 4: 14Ã—14 â†’ 7Ã—7
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d(1),
        )
        self.proj = nn.Linear(128, output_dim)
    
    def forward(self, x):
        x = self.conv(x)        # [B, 128, 1, 1]
        x = x.flatten(1)        # [B, 128]
        return self.proj(x)     # [B, 128]

# Stats:
# - Params: ~85K (7000Ã— smaller than CLIP!)
# - Memory: ~0.3MB
# - Latency: ~5ms
# - Accuracy: ~50% ImageNet (acceptable for UI/scene understanding)
Comparison Table
ModelParamsMemoryLatencyImageNet AccUse CaseCLIP ViT-B/32150M600MB500ms85%Server-sideMobileNetV3-Small1.5M6MB20ms67%RecommendedEfficientNet-Lite04.6M18MB30ms75%High-end phonesTinyVisionEncoder85K0.3MB5ms50%Old devices

ğŸ—ï¸ Updated Mobile Architecture
Event Encoder (Mobile-Optimized)
fimg=MobileNetV3frozen(vt)âˆˆR128(wasÂ 512)fsym=Embed(st)âˆˆR32(reducedÂ fromÂ 64)ftime=T(et,Ï„now)âˆˆR14he=[fimg;fsym;ftime]âˆˆR174ze=MLPÎ¦(he)âˆˆR64\begin{align}
\mathbf{f}_{\text{img}} &= \text{MobileNetV3}_{\text{frozen}}(v_t) \in \mathbb{R}^{128} \quad \text{(was 512)} \\
\mathbf{f}_{\text{sym}} &= \text{Embed}(s_t) \in \mathbb{R}^{32} \quad \text{(reduced from 64)} \\
\mathbf{f}_{\text{time}} &= \mathcal{T}(e_t, \tau_{\text{now}}) \in \mathbb{R}^{14} \\
\mathbf{h}_e &= [\mathbf{f}_{\text{img}}; \mathbf{f}_{\text{sym}}; \mathbf{f}_{\text{time}}] \in \mathbb{R}^{174} \\
\mathbf{z}_e &= \text{MLP}_\Phi(\mathbf{h}_e) \in \mathbb{R}^{64}
\end{align}fimgâ€‹fsymâ€‹ftimeâ€‹heâ€‹zeâ€‹â€‹=MobileNetV3frozenâ€‹(vtâ€‹)âˆˆR128(wasÂ 512)=Embed(stâ€‹)âˆˆR32(reducedÂ fromÂ 64)=T(etâ€‹,Ï„nowâ€‹)âˆˆR14=[fimgâ€‹;fsymâ€‹;ftimeâ€‹]âˆˆR174=MLPÎ¦â€‹(heâ€‹)âˆˆR64â€‹â€‹
MLP architecture (smaller):
pythonMLP_Î¦ = nn.Sequential(
    nn.Linear(174, 128),
    nn.GELU(),
    nn.Linear(128, 64),
    nn.GELU(),
    nn.Linear(64, 64),
)
# Params: 174Ã—128 + 128Ã—64 + 64Ã—64 = 35K (was 380K)
```

**Total mobile model size:**
- Vision encoder: 1.5M (frozen)
- Event encoder: 35K
- Memory encoder: 25K (reduced)
- Op classifier: 10K
- **Total trainable: ~70K params = 280KB**

Can run inference in **<50ms** on modern phones!

---

## ğŸ§  **Intuition Behind the Loss Functions**

### **Why We Need Each Loss Component**

The total loss is:
$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{rank}} + \lambda_m \mathcal{L}_{\text{margin}} + \lambda_o \mathcal{L}_{\text{op}} + \lambda_c \mathcal{L}_{\text{consist}}
$$

Let me explain **why each loss exists** with concrete examples:

---

### **Loss 1: Contrastive Ranking Loss** 
$$
\mathcal{L}_{\text{rank}} = -\log \frac{\sum_{m_j \in \mathcal{M}_t^+} \exp(s(e_t, m_j)/\tau)}{\sum_{m_j \in \mathcal{M}_t^+} \exp(s(e_t, m_j)/\tau) + \sum_{m_k \in \mathcal{M}_t^-} \exp(s(e_t, m_k)/\tau)}
$$

#### **Intuition: "Pull relevant memories close, push irrelevant ones away"**

**Scenario:**
```
Event: User opens Gmail and reads email from "boss@company.com"

Memory Bank:
m1: "User's boss is John Smith (john@company.com)"     â† SHOULD UPDATE
m2: "User prefers reading emails in the morning"        â† SHOULD UPDATE  
m3: "User's favorite food is pizza"                     â† IGNORE
m4: "User follows @elonmusk on Twitter"                 â† IGNORE
```

**What the loss does:**
1. **Positive pairs** (event â†’ m1, m2): Make $s(e_t, m_1)$ and $s(e_t, m_2)$ **high**
2. **Negative pairs** (event â†’ m3, m4): Make $s(e_t, m_3)$ and $s(e_t, m_4)$ **low**

**Why softmax?** 
- Relative scoring: "m1 should rank higher than m3"
- Handles variable number of positives (could be 1 or 5 memories to update)
- Temperature $\tau$ controls sharpness (low $\tau$ = winner-takes-all, high $\tau$ = smooth)

**Why it fails alone:**
Sometimes negatives are **deceptively similar**:
```
Event: "User opens Gmail"
m_hard: "User opened Outlook yesterday"  â† High semantic similarity but WRONG!
```

This memory mentions email but is about a **different app**. Contrastive loss alone might not separate them well.

---

### **Loss 2: Hard Negative Margin Loss**
$$
\mathcal{L}_{\text{margin}} = \sum_{m_j \in \mathcal{M}_t^+} \sum_{m_k \in \mathcal{M}_t^{\text{hard}}} \max(0, \gamma + s(e_t, m_k) - s(e_t, m_j))
$$

#### **Intuition: "Force a safety gap between confusing cases"**

**Scenario:**
```
Event: "User opened Bank of America app, balance: $5,234"

Memory Bank:
m1: "User's Bank of America balance: $4,800"           â† UPDATE (correct)
m2: "User's Chase bank balance: $12,500"               â† IGNORE (different bank)
m3: "User checked bank balance 3 days ago"             â† UPDATE (related)
```

**Problem:** $m_2$ has high semantic similarity (both about bank balance) but should NOT be updated.

**What margin loss does:**
$$
\max(0, 0.2 + \underbrace{s(e_t, m_2)}_{\text{hard negative}} - \underbrace{s(e_t, m_1)}_{\text{positive}})
$$

Forces: $s(e_t, m_1) > s(e_t, m_2) + 0.2$

**Why $\gamma = 0.2$?**
- Creates a **buffer zone** between classes
- Prevents model from being "barely right" (scores like 0.51 vs 0.49)
- Margin is learned empirically (0.1â€“0.3 works well)

**Analogy:** Like requiring students to score 70% (not 50%) to passâ€”forces clear understanding, not lucky guesses.

---

### **Loss 3: Operation Classification Loss**
$$
\mathcal{L}_{\text{op}} = -\sum_{m_i \in \mathcal{M}_t^{\text{candidates}}} \log p(o_i^* | e_t, m_i)
$$

#### **Intuition: "Not just WHICH memories, but WHAT to do with them"**

**Scenario:**
```
Event: "User's boss John Smith resigned. New boss: Sarah Chen"

Memory Bank:
m1: "User's boss is John Smith"                â†’ DELETE (outdated)
m2: "John Smith hired user in 2019"            â†’ UPDATE (add context)
m3: "User reports to engineering department"   â†’ UPDATE (still valid, add detail)
```

**Problem:** Ranking alone doesn't tell you:
- Should we DELETE m1 or UPDATE it?
- Are m1 and m2 talking about the same thing (MERGE)?

**What this loss does:**
Predicts: $p(\text{UPDATE} | e_t, m_1)$, $p(\text{DELETE} | e_t, m_1)$, etc.

**Why add this?**
1. **Multi-task learning**: Helps encoder learn richer representations
2. **Interpretability**: Can explain "why" a memory was selected
3. **Error correction**: If ranking puts DELETE candidate at top, op classifier can catch it

**Example failure without this loss:**
```
Model ranks highly: "User's old phone number: 555-1234"
But event is: "User got new number: 555-9999"
â†’ Should DELETE old number, not UPDATE it!
```

Operation classifier learns: "If event contradicts memory â†’ DELETE, not UPDATE"

---

### **Loss 4: Consistency Regularization**
$$
\mathcal{L}_{\text{consist}} = \sum_{(m_i, m_j) \in \mathcal{G}} \|\mathbf{z}_{m_i} - \mathbf{z}_{m_j}\|_2^2 \cdot \mathbb{1}[o_i = o_j = \text{UPDATE}]
$$

#### **Intuition: "Related memories should move together"**

**Scenario:**
```
Event: "User moved from San Francisco to Austin"

Memory Bank:
m1: "User lives in San Francisco"              â† UPDATE
m2: "User's work commute: SF â†’ Mountain View"  â† UPDATE
m3: "User's favorite SF restaurant: Tartine"   â† UPDATE (context changed)
m4: "User likes pizza"                         â† IGNORE (unrelated)
```

**Problem:** If you update m1 but not m2, memory bank becomes **inconsistent**:
```
After update:
m1: "User lives in Austin"         âœ“
m2: "User commutes SF â†’ MTV"       âœ— (contradiction!)
What this loss does:

Finds memories that share entities: entity(m1)âˆ©entity(m2)={"userÂ location"}\text{entity}(m_1) \cap \text{entity}(m_2) = \{\text{"user location"}\}
entity(m1â€‹)âˆ©entity(m2â€‹)={"userÂ location"}
If both get updated, pull their embeddings closer: âˆ¥zm1âˆ’zm2âˆ¥2\|\mathbf{z}_{m_1} - \mathbf{z}_{m_2}\|^2
âˆ¥zm1â€‹â€‹âˆ’zm2â€‹â€‹âˆ¥2 small

Encourages model to co-update related memories

Why it matters:

Prevents cascading errors (one update breaks other memories)
Learns implicit entity graphs (boss â†’ company â†’ location)
Improves long-term memory coherence

Analogy: Like updating a spreadsheetâ€”if you change someone's address, you should also update their shipping info, tax forms, etc.

ğŸ“Š Training Data Generation (Detailed)
Challenge: We Don't Have Human Labels
We need:

Events with ground truth "which memories to update"
But labeling by hand is expensive and subjective

Solution: Bootstrap from LLM interactions

Stage 1: Cold Start (Heuristic Routing)
pythondef heuristic_router(event, memory_bank, K=20):
    """
    Simple rule-based system to get initial candidates.
    Used before we have trained models.
    """
    candidates = []
    
    # Rule 1: Entity overlap
    event_entities = extract_entities(event)  # {app, person, location, ...}
    for memory in memory_bank:
        memory_entities = extract_entities(memory.text)
        overlap = len(event_entities & memory_entities)
        if overlap > 0:
            candidates.append((memory, overlap * 10))  # score
    
    # Rule 2: Temporal recency
    current_time = event.timestamp
    for memory in memory_bank:
        days_since_update = (current_time - memory.last_update).days
        if days_since_update < 30:  # active memories
            recency_score = 1.0 / (1 + days_since_update)
            candidates.append((memory, recency_score * 5))
    
    # Rule 3: Activity type
    if event.activity in ['email', 'message']:
        # Retrieve communication-related memories
        comm_memories = [m for m in memory_bank if 'communication' in m.tags]
        candidates.extend([(m, 3) for m in comm_memories])
    
    # Aggregate scores
    memory_scores = {}
    for memory, score in candidates:
        memory_scores[memory.id] = memory_scores.get(memory.id, 0) + score
    
    # Return top-K
    top_k = sorted(memory_scores.items(), key=lambda x: -x[1])[:K]
    return [memory_bank.get(mem_id) for mem_id, _ in top_k]
Key idea: Start with interpretable rules that are 60-70% accurate. Good enough to collect initial data.

Stage 2: LLM Annotation
pythondef collect_llm_labels(events, memory_bank, num_samples=10000):
    """
    Use LLM to generate ground truth labels for training.
    """
    training_data = []
    
    for event in tqdm(events[:num_samples]):
        # Get candidates from heuristic
        candidates = heuristic_router(event, memory_bank, K=20)
        
        # Query LLM for ground truth
        prompt = f"""
You are a memory management system. Given an event and candidate memories:

EVENT:
- Activity: {event.activity}
- App: {event.app}
- Entities: {event.entities}
- Summary: {event.text_summary}
- Time: {event.timestamp}

CANDIDATE MEMORIES (choose which to update):
{format_memories(candidates)}

For each memory, decide:
- UPDATE: Event adds new information to this memory
- MERGE: This memory should be combined with another
- DELETE: This memory is now outdated/wrong
- IGNORE: Event is not relevant to this memory

Respond in JSON:
{{
  "updates": [memory_ids],
  "merges": [[id1, id2], ...],
  "deletes": [memory_ids],
  "ignores": [memory_ids],
  "reasoning": "..."
}}
"""
        
        response = query_gpt4(prompt)
        llm_decision = json.loads(response)
        
        # Construct training example
        training_example = {
            'event': event,
            'candidates': candidates,
            'labels': {
                'positive_indices': llm_decision['updates'] + 
                                   [m for pair in llm_decision['merges'] for m in pair],
                'negative_indices': llm_decision['ignores'],
                'operations': construct_operation_labels(llm_decision),
            }
        }
        
        training_data.append(training_example)
        
        # CRITICAL: Actually execute the updates to evolve memory bank
        memory_bank = execute_operations(memory_bank, llm_decision, event)
    
    return training_data, memory_bank
Key steps:

Start with heuristic candidates (cheap, fast)
Ask LLM to label a subset (10K examples = ~$100 API cost)
Execute the operations so memory bank evolves realistically
This creates a trajectory of memory states for training


Stage 3: Hard Negative Mining
pythondef mine_hard_negatives(training_data, memory_bank):
    """
    Find challenging negative examples to improve model robustness.
    """
    for example in training_data:
        event = example['event']
        positive_mems = example['labels']['positive_indices']
        
        # Type 1: High semantic similarity but wrong
        semantic_sim = compute_embeddings(event, memory_bank)
        top_similar = semantic_sim.topk(50).indices
        
        hard_negatives_semantic = [
            mem_id for mem_id in top_similar 
            if mem_id not in positive_mems
        ][:5]
        
        # Type 2: Same entity but wrong temporal context
        event_entities = extract_entities(event)
        temporal_confounders = [
            mem for mem in memory_bank
            if (len(extract_entities(mem) & event_entities) > 0 and
                abs((mem.last_update - event.timestamp).days) > 7 and
                mem.id not in positive_mems)
        ][:5]
        
        # Type 3: Frequently updated memories (popularity bias)
        popular_mems = sorted(memory_bank, key=lambda m: -m.update_count)[:10]
        popular_negatives = [
            mem.id for mem in popular_mems 
            if mem.id not in positive_mems
        ][:3]
        
        # Add to training example
        example['labels']['hard_negatives'] = (
            hard_negatives_semantic +
            [m.id for m in temporal_confounders] +
            popular_negatives
        )
    
    return training_data
Why hard negatives matter:

Easy negatives: "User opened Gmail" vs "User likes pizza" â†’ model learns trivial features
Hard negatives: "User opened Gmail" vs "User opened Outlook" â†’ model learns fine-grained distinctions


Stage 4: Data Augmentation
pythondef augment_training_data(training_data):
    """
    Synthetically increase training diversity.
    """
    augmented = []
    
    for example in training_data:
        # Original example
        augmented.append(example)
        
        # Augmentation 1: Time shift
        # Simulate "what if this happened 1 week earlier?"
        shifted = copy.deepcopy(example)
        shifted['event'].timestamp -= timedelta(days=7)
        # Re-encode temporal features
        augmented.append(shifted)
        
        # Augmentation 2: Entity substitution
        # Replace "John" with "Sarah" to test generalization
        if 'person' in example['event'].entities:
            substituted = copy.deepcopy(example)
            old_person = list(example['event'].entities['person'])[0]
            new_person = random.choice(PERSON_NAMES)
            substituted['event'] = substitute_entity(
                substituted['event'], old_person, new_person
            )
            augmented.append(substituted)
        
        # Augmentation 3: Negative sampling variation
        # Different random negatives to prevent overfitting
        varied = copy.deepcopy(example)
        varied['labels']['negative_indices'] = random.sample(
            [m for m in memory_bank if m.id not in example['labels']['positive_indices']],
            k=10
        )
        augmented.append(varied)
    
    return augmented

Complete Data Pipeline
pythondef generate_complete_training_dataset(
    raw_events,           # User's event stream (unlabeled)
    initial_memory_bank,  # Starting state
    num_bootstrap=10000,  # How many LLM labels to collect
    num_augment=3        # Augmentation multiplier
):
    """
    End-to-end pipeline to generate training data from scratch.
    """
    
    print("Stage 1: Heuristic cold start...")
    heuristic_candidates = []
    for event in raw_events[:num_bootstrap]:
        candidates = heuristic_router(event, initial_memory_bank)
        heuristic_candidates.append({
            'event': event,
            'candidates': candidates
        })
    
    print("Stage 2: LLM annotation...")
    llm_labeled_data, evolved_memory_bank = collect_llm_labels(
        raw_events[:num_bootstrap],
        initial_memory_bank
    )
    print(f"Collected {len(llm_labeled_data)} labeled examples")
    print(f"LLM API cost: ~${len(llm_labeled_data) * 0.01:.2f}")
    
    print("Stage 3: Hard negative mining...")
    data_with_hard_negs = mine_hard_negatives(
        llm_labeled_data,
        evolved_memory_bank
    )
    
    print("Stage 4: Data augmentation...")
    augmented_data = augment_training_data(data_with_hard_negs)
    print(f"Augmented to {len(augmented_data)} examples")
    
    print("Stage 5: Entity graph construction...")
    for example in augmented_data:
        # Build co-occurrence graph for consistency loss
        example['entity_graph'] = build_entity_cooccurrence_graph(
            example['candidates']
        )
    
    print("Stage 6: Train/val/test split...")
    # Split by users (not events) to test generalization
    users = set(e['event'].user_id for e in augmented_data)
    train_users = random.sample(users, int(0.7 * len(users)))
    val_users = random.sample(users - set(train_users), int(0.15 * len(users)))
    test_users = users - set(train_users) - set(val_users)
    
    train_data = [e for e in augmented_data if e['event'].user_id in train_users]
    val_data = [e for e in augmented_data if e['event'].user_id in val_users]
    test_data = [e for e in augmented_data if e['event'].user_id in test_users]
    
    print(f"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")
    
    return {
        'train': train_data,
        'val': val_data,
        'test': test_data,
        'evolved_memory_bank': evolved_memory_bank,
    }


# Usage
dataset = generate_complete_training_dataset(
    raw_events=load_user_events(),  # Load from logs
    initial_memory_bank=MemoryBank(),
    num_bootstrap=10000,
    num_augment=3
)

# Save for training
torch.save(dataset, 'memory_routing_dataset.pt')

Example Training Sample
python{
  'event': {
    'timestamp': '2024-12-15 14:32:00',
    'app': 'gmail',
    'activity': 'read_email',
    'entities': {'person': ['boss@company.com'], 'org': ['Acme Corp']},
    'text_summary': 'Read email from boss about Q4 targets',
    'image': <screenshot tensor>,
  },
  
  'candidates': [
    Memory(id=42, text='User works at Acme Corp', update_count=5),
    Memory(id=17, text='Boss is John Smith (boss@company.com)', update_count=12),
    Memory(id=88, text='Q3 targets were $2M revenue', update_count=3),
    # ... 17 more
  ],
  
  'labels': {
    'positive_indices': [17, 88],  # Boss info + revenue context
    'negative_indices': [3, 7, 9, 12, 15, ...],  # 15 unrelated memories
    'hard_negatives': [23, 54, 91],  # Confusing cases
    'operations': {
      17: OP_UPDATE,    # Add "discussed Q4 targets"
      88: OP_UPDATE,    # Add Q4 context
      3: OP_IGNORE,
      7: OP_IGNORE,
      # ...
    },
    'entity_graph': [(17, 88)],  # Boss and company are related
  }
}

ğŸ“ˆ Data Statistics
For a production system, you'd want:
MetricTargetTotal examples50K-100KUsers100-500Events per user500-1000Avg memories per event20 candidatesPositive rate10-15% (2-3 per event)Hard negatives per event3-5LLM labeling cost$500-1000
Efficiency note: After initial 10K LLM labels, use the trained model to pseudo-label the rest, then spot-check with LLM (active learning). Reduces cost 10Ã—.
